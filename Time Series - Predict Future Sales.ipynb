{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Project Description# \n\nThis project is the final project for the \"How to win a data science competition\" Coursera course.\n\nThis project is provided with a challenging time-series dataset consisting of historical daily sales data. The task is to predict total sales for every product sold in every store in the next month. Note that the list of shops and products slightly changes every month. Creating a robust model that can handle such situations is part of the challenge."},{"metadata":{},"cell_type":"markdown","source":"# Project Pipeline\n\n* load data\n* remove outliers and handle abnormal values\n* handle data leakage to avoid over-generalization\n* engineer new features, such as revenue, lagged features, rolling stats, et cetera\n* explore data trend and visualize\n* re-structure data to fit into models\n* prepare train/validation/test datasets\n* set model performance evaluators\n* explore different ways to forecast timeseries data:\n*     1. AR/MA models\n*     2. LSTM model\n*     3. Ensemble method (with Linear regression/XGBoost/Random Forest models)"},{"metadata":{},"cell_type":"markdown","source":"# **Data extract and preprocessing**"},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport datetime\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pylab\n\npd.set_option('display.width', 400)\npd.set_option('display.max_columns', 10)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"174b366d-f28f-4784-8672-4f20586618ae","_cell_guid":"f5bf4fbb-1ddd-440c-984b-a5049367cabe","trusted":true},"cell_type":"code","source":"sales_train = pd.read_csv('../input/competitive-data-science-predict-future-sales/sales_train.csv')\nitems = pd.read_csv('../input/competitive-data-science-predict-future-sales/items.csv')\nitem_cat = pd.read_csv('../input/competitive-data-science-predict-future-sales/item_categories.csv')\nshops = pd.read_csv('../input/competitive-data-science-predict-future-sales/shops.csv')\ntest_sub = pd.read_csv('../input/competitive-data-science-predict-future-sales/test.csv').set_index('ID')\nsample_sub = pd.read_csv('../input/competitive-data-science-predict-future-sales/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Handle outliers and abnormal values**"},{"metadata":{"_uuid":"189cde4c-61af-420b-9b6d-c50512780731","_cell_guid":"c5528581-ba24-4d26-a2e6-f57e5b943c6c","trusted":true},"cell_type":"code","source":"# check if there are outliers\nsns.boxplot(x='item_price',data=sales_train)\nsales_train = sales_train[sales_train['item_price']<10000]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.boxplot(x='item_cnt_day',data=sales_train)\nsales_train = sales_train[sales_train['item_cnt_day']<1000]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2de21370-b801-4b45-b953-8fd20bdbc279","_cell_guid":"16508f8c-1473-479b-9c7b-33dc130f2ddb","trusted":true},"cell_type":"code","source":"# it looks there are negative numbers in column item_cnt_day, let's take a look\nnegative_cnt = sales_train[sales_train['item_cnt_day']<0].sort_values(by='item_cnt_day',ascending=True)\ng = sns.countplot(x=negative_cnt['item_cnt_day'],palette='viridis',order=negative_cnt[\"item_cnt_day\"].value_counts().index)#[negative_cnt['item_cnt_day']<-1])\ng.set(yscale='log')\nplt.title('Negative Daily Item Count Distribution')\nfor p in g.patches:\n    height = p.get_height()\n    g.text(p.get_x() + p.get_width() / 2.,\n            height + 3,\n            '{:.0f}'.format(height),\n            ha=\"center\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are more than 7k data entries with negative daily item count. In the project this data field is described as \"number of products sold\". Does a negative number indicate a return&refund?\nIt's not clear what's the meaning of negative numbers in item_cnt_day yet. For now let's treat it as outliers and remove it from the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train = sales_train[sales_train['item_cnt_day']>0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"45baa3f1-3f0d-452a-a6f2-98dd28e94cee","_cell_guid":"2ca38c04-5ba7-4f34-a5ae-400095cd98b6","trusted":true},"cell_type":"code","source":"# let's check if there are negative values in the item_price column\nprint(sales_train[sales_train['item_price']<0])\n# we can see there is one item with negative price. Let's fill it with median price of all the other items with the same item_id from the same shop on the same day (or in the same month if there is no other items sold from the same day)\nm = sales_train[(sales_train['shop_id']==32)&(sales_train['item_id']==2973)&(sales_train['date_block_num']==4)&(sales_train['item_price']>0)]['item_price'].median()\nsales_train.loc[sales_train['item_price']<0,'item_price'] = m","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"3271a595-f429-4617-b8bc-43eb7c9e7679","_cell_guid":"48412009-fc1a-40c7-bbf9-16b24b4455ac","trusted":true},"cell_type":"code","source":"# now sales_train data is in good shape, let's look at the data from the other files. I don't understand Russian, but there are a few name fields that show similar strings, let's manually handle these given the small data size\ndup_shop = shops.loc[shops['shop_id'].isin([0,1,10,11,57,58])]\nprint(dup_shop['shop_name']) #so we can see 0/57, 1/58, 10/11 are dups\n\nsales_train.loc[sales_train['shop_id']==0,'shop_id']=57\ntest_sub.loc[test_sub['shop_id']==0,'shop_id']=57\nsales_train.loc[sales_train['shop_id']==1,'shop_id']=58\ntest_sub.loc[test_sub['shop_id']==1,'shop_id']=58\nsales_train.loc[sales_train['shop_id']==10,'shop_id']=11\ntest_sub.loc[test_sub['shop_id']==10,'shop_id']=11","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# item_cat data looks fine. Let's look at items data.\nitems_40 = items[items['item_category_id']==40]\nitems_40.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most of items data looks fine, though it looks some of the items name contain special character, we don't know if the special characters are legit part of item names, or errors. \nAs I won't do anyting with text features (also I don't understand the Russian text in this case), for now I will leave it as is."},{"metadata":{"_uuid":"951fd84d-1485-49ff-8560-19111003b0d0","_cell_guid":"b5e4fda4-738d-453f-a159-dd1c5b07a5f5","trusted":true},"cell_type":"code","source":"# now that individual tables are handled, let's merge them together. since we don't need the descriptive features (shop name, item name, etc), we only need to join table items to sales_train, so that sales_train can have item_category_id info\nsales_train = sales_train.join(items,on='item_id',rsuffix='_').drop(['item_id_','item_name'],axis=1)\nsales_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Data leakage**\n\nWe want to eliminate the issue of over-generalization, as there are some shops/items that appear in trainset but didn't appear in testset. We want the model to only focus on shops/items that are included in the testdata.\n"},{"metadata":{"_uuid":"fafd1265-46f6-4674-9813-92e42ef0475a","_cell_guid":"b952372c-7636-43c1-8bb9-29fb35b28764","trusted":true},"cell_type":"code","source":"test_shop_ids = test_sub['shop_id'].unique()\ntest_item_ids = test_sub['item_id'].unique()\n# Only shops that exist in test set.\nsales_train_lk = sales_train[sales_train['shop_id'].isin(test_shop_ids)]\n# Only items that exist in test set.\nsales_train_lk = sales_train_lk[sales_train_lk['item_id'].isin(test_item_ids)]\nprint('train set size before leaking:',sales_train.shape)\nprint('train set size after leaking:',sales_train_lk.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Feature generation**"},{"metadata":{"_uuid":"2ef67086-9400-4c4d-8d59-99d47c7604c5","_cell_guid":"3311e313-cf47-4fa8-89d0-d26312ff2750","trusted":true},"cell_type":"code","source":"# let's convert the date column to the right format\nsales_train_lk['date'] = sales_train_lk['date'].apply(lambda x: datetime.datetime.strptime(x,'%d.%m.%Y'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add a revenue feature\nsales_train_lk['revenue'] = sales_train_lk['item_price']*sales_train_lk['item_cnt_day']","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"5d4d73c7-65e3-481a-8578-cd279f3d6a65","_cell_guid":"33a0c989-973e-4b98-aeed-fbcf01f85bbc","trusted":true},"cell_type":"code","source":"monthly_train_lk = sales_train_lk.groupby(['date_block_num','shop_id','item_category_id','item_id'],as_index=False)#['date','item_cnt_day','item_price'].agg({'date':['min','max'],'item_cnt_day':'sum','item_price':'mean'}).reset_index()\nmonthly_train_lk = monthly_train_lk.agg({'item_price':['sum','mean'],\n                                         'item_cnt_day':['sum','mean','count'],\n                                         'revenue':['sum','mean']})\nmonthly_train_lk.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rename features\nmonthly_train_lk.columns = ['date_block_num','shop_id','item_category_id','item_id','total_item_price','avg_item_price','total_item_cnt','avg_item_cnt','total_txn_cnt','total_revenue','avg_revenue']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"monthly_train_lk.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There are about 6k records with total_item_cnt>20, about 1% of the total data size, I will treat these as outliers and remove these records from the dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"monthly_train_lk = monthly_train_lk[monthly_train_lk['total_item_cnt']<=20]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"monthly_train_lk.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Create placeholder for the missing records of combination of date_block_num, shop_id and item_id"},{"metadata":{"trusted":true},"cell_type":"code","source":"# build a data set with all the possible combinations of ['date_block_num','shop_id','item_id'] so we won't have missing shop_id or item_id that exist in testdata but not in traindata.\ntrain_shop_ids = monthly_train_lk['shop_id'].unique()\ntrain_item_ids = monthly_train_lk['item_id'].unique()\ntemp_df = []\nfor i in range(len(monthly_train_lk['date_block_num'].unique())):\n    for shop in train_shop_ids:\n        for item in train_item_ids:\n            temp_df.append([i,shop,item])\ntemp_df = pd.DataFrame(temp_df,columns=['date_block_num','shop_id','item_id'])\nprint(temp_df.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# merge the placeholder set with the train dataset, and fill missing recodes with 0\nmonthly_train_lk = pd.merge(temp_df,monthly_train_lk,on=['date_block_num','shop_id','item_id'],how='left')\nmonthly_train_lk.fillna(0,inplace=True)\nprint(monthly_train_lk.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's generate two more columns based on date_block_num: month and year\nmonthly_train_lk['month'] = monthly_train_lk['date_block_num'].apply(lambda x: ((x%12)+1))\nmonthly_train_lk['year'] = monthly_train_lk['date_block_num'].apply(lambda x: ((x//12)+2013))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# unitary item price\n# monthly_train_lk.columns = ['date_block_num','shop_id','item_category_id','item_id','total_item_price','avg_item_price','total_item_cnt','avg_item_cnt','total_txn_cnt','total_revenue','avg_revenue']\nmonthly_train_lk['unit_item_price'] = monthly_train_lk['total_revenue']//monthly_train_lk['total_item_cnt']\nmonthly_train_lk['unit_item_price'].fillna(0,inplace=True)\nmonthly_train_lk[np.isinf(monthly_train_lk)] = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# min and max of unitary item price over the whole sales period\nminmax_price = monthly_train_lk.groupby(['item_id'],as_index=False).agg({'unit_item_price':[np.min,np.max]})\nminmax_price.columns = ['item_id','min_unit_item_price','max_unit_item_price']\nmonthly_train_lk = pd.merge(monthly_train_lk,minmax_price,on='item_id',how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# unit price change compared to min and max unitary price\nmonthly_train_lk['price_change+'] = monthly_train_lk['unit_item_price'] - monthly_train_lk['min_unit_item_price']\nmonthly_train_lk['price_change-'] = monthly_train_lk['max_unit_item_price'] - monthly_train_lk['unit_item_price']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lagged features\nlags = [1,2,3,6,12]\nfor lag in lags:\n    col = ('total_item_cnt_lag_%s' % lag)\n    monthly_train_lk[col] = monthly_train_lk.sort_values('date_block_num').groupby(['shop_id','item_category_id','item_id'])['total_item_cnt'].shift(lag)\n    monthly_train_lk[col].fillna(0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# item sale trend based on lagged features\nmonthly_train_lk['trend'] = monthly_train_lk['total_item_cnt']\nfor lag in lags:\n    col = ('total_item_cnt_lag_%s' % lag)\n    monthly_train_lk['trend'] -= monthly_train_lk[col]\nmonthly_train_lk['trend'] /= (len(lags)+1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# rolling window based features off total_item_cnt\n# define features\ncnt_min = lambda x: x.rolling(window=3,min_periods=1).min()\ncnt_max = lambda x: x.rolling(window=3,min_periods=1).max()\ncnt_avg = lambda x: x.rolling(window=3,min_periods=1).mean()\ncnt_std = lambda x: x.rolling(window=3,min_periods=1).std()\nfunc_list = [cnt_min,cnt_max,cnt_avg,cnt_std]\nfunc_name = ['rolling_min','rolling_max','rolling_avg','rolling_std']\n\nfor i in range(len(func_list)):\n    monthly_train_lk[('total_item_cnt_%s'%func_name[i])] = monthly_train_lk.sort_values('date_block_num').groupby(['shop_id','item_category_id','item_id'])['total_item_cnt'].apply(func_list[i])\n\nmonthly_train_lk['total_item_cnt_rolling_std'].fillna(0,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# create the final prediction feature 'item_cnt_next_month'\nmonthly_train_lk['item_cnt_next_month'] = monthly_train_lk.sort_values('date_block_num').groupby(['shop_id','item_id'])['total_item_cnt'].shift(-1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# take a look at the data with all features generated\nmonthly_train_lk.head().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Exploratory data analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"monthly_train_lk.describe().T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's first look at overall monthly sales data\nmonth_sum = monthly_train_lk.groupby(['date_block_num','year','month'],as_index=False)['total_item_cnt'].sum()\nmonth_sum_pivot = month_sum.pivot(values='total_item_cnt',columns='year',index='month')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(12,5))\nsns.lineplot(x='date_block_num',y='total_item_cnt',data=month_sum,label='raw data')\nplt.plot(month_sum['total_item_cnt'].rolling(window=3,center=False).mean(),label='rolling mean')\nplt.plot(month_sum['total_item_cnt'].rolling(window=3,center=False).std(),label='rolling std')\nplt.title('Trend of Items Sold Monthly')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It looks there is a year-over-year upward trend with obvious spike by the end of each year. Let's take a closer look by breaking down data into each years and compare YOY."},{"metadata":{"trusted":true},"cell_type":"code","source":"fig = plt.figure(figsize=(12,5))\nax = fig.add_subplot()\ncolors = [plt.cm.viridis(i/float(len(month_sum_pivot.columns))) for i in range(len(month_sum_pivot.columns))]\nx = month_sum_pivot.index\nfor col,color in zip(month_sum_pivot.columns,colors):\n    y = month_sum_pivot[col]\n    ax.plot(x,y,label=col,c=color)\n    x_annotate = x[0]\n    y_annotate = month_sum_pivot.iloc[0][col]\n    ax.text(x_annotate-0.4,y_annotate,col,fontsize=8,c=color)\nax.set_xlabel('Months', fontsize = 13)\nax.set_ylabel('Items Sold Monthly', fontsize = 13)\nax.set_title('Monthly Sales by Year (2013-2015)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"There is an upward trend year over year: 2014 on average is better than 2013, and 2015 better than 2014. And at the end of each year there is an obvious spike of sales."},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's look at trend of revenue\nmonth_revenue = monthly_train_lk.groupby(['date_block_num','year','month'],as_index=False)['total_revenue'].sum()\n\nplt.figure(figsize=(12,5))\nsns.lineplot(x='date_block_num',y='total_revenue',data=month_revenue).set_title('Trend of Monthly Revenue')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Revenue shows the similar trend as items sold."},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's then look at sales data at category level\ncategory_sum = monthly_train_lk.groupby(['item_category_id'],sort=False)['total_item_cnt'].agg([('total_item_cnt','sum')]).reset_index().sort_values(by='total_item_cnt',ascending=False)\n#category_sum = category_sum.sort_values(by='total_item_cnt',ascending=False)\nprint(category_sum.head())\nplt.figure(figsize=(12,5))\nsns.barplot(x='item_category_id',y='total_item_cnt',data=category_sum,palette='viridis').set_title('Monthly Sales by Categories')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"category_rev_sum = monthly_train_lk.groupby(['item_category_id'],sort=False)['total_revenue'].agg([('total_revenue','sum')]).reset_index().sort_values(by='total_revenue',ascending=False)\nprint(category_rev_sum.head())\nplt.figure(figsize=(12,5))\nsns.barplot(x='item_category_id',y='total_revenue',data=category_rev_sum,palette='viridis').set_title('Monthly Revenue by Categories')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Looks a few number of categories take up a large portion of the total sell count. Yet top revenue contributors are not necessarily the most popular categories."},{"metadata":{"trusted":true},"cell_type":"code","source":"shop_mean = monthly_train_lk.groupby(['shop_id'],as_index=False)['total_item_cnt'].mean()\nshop_sum = monthly_train_lk.groupby(['shop_id'],as_index=False)['total_item_cnt'].sum()\nprint(shop_sum.head())\nplt.figure(figsize=(12,5))\nsns.barplot(x='shop_id',y='total_item_cnt',data=shop_sum,palette='viridis').set_title('Monthly Sales by Shops')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"shop_rev_sum = monthly_train_lk.groupby(['shop_id'],as_index=False)['total_revenue'].sum()\nprint(shop_rev_sum.head())\nplt.figure(figsize=(12,5))\nsns.barplot(x='shop_id',y='total_revenue',data=shop_rev_sum,palette='viridis').set_title('Monthly Revenue by Shops')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Most shops have a similar sell count. And shops that sell the most number of items are also top revenue contributors."},{"metadata":{"trusted":true},"cell_type":"code","source":"del sales_train\ndel items\ndel item_cat\ndel shops\ndel negative_cnt\ndel dup_shop\ndel items_40\ndel test_shop_ids\ndel test_item_ids\ndel train_shop_ids\ndel train_item_ids\ndel temp_df\ndel month_sum\ndel month_sum_pivot\ndel month_revenue\ndel category_sum\ndel category_rev_sum\ndel shop_mean\ndel shop_sum\ndel shop_rev_sum\n\nimport gc \ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Modeling and prediction"},{"metadata":{},"cell_type":"markdown","source":"**Define a performance evaluation visualization function**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# define a performance evaluation function and visualize the performance of different models\n# The closer the points are to the middle dashed line the better are the predictions.\nfrom numpy.polynomial.polynomial import polyfit\n\ndef model_performance_sc_plot(predictions,labels,title):\n    min_val = max(max(predictions),max(labels))\n    max_val = min(min(predictions),min(labels))\n    performance = pd.DataFrame({'Label':labels,'Prediction':predictions})\n    #performance = performance.astype(dtype={'Label':float,'Prediction':float})\n    \n    b, m = polyfit(performance['Prediction'], performance['Label'], 1)\n    # plot the data\n    plt.figure(figsize=(8,6))\n    plt.plot([min_val,max_val],[min_val,max_val],'m--')\n    plt.plot(performance['Prediction'],performance['Label'],'.')\n    plt.plot(performance['Prediction'],b+m*performance['Prediction'],'-')\n    plt.title(title)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Part I : ARIMA***\n\nI start with ARIMA mmodels because it's simple to implement without any parameter tuning, and quick to run."},{"metadata":{"trusted":true},"cell_type":"code","source":"arima_data_df = monthly_train_lk.groupby(['date_block_num']).agg({'total_item_cnt':'sum'})\narima_data = list(arima_data_df['total_item_cnt'].values)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Dickey-Fuller test**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# perform dickey-fuller test to assess whether data is stationary\nfrom statsmodels.tsa.stattools import adfuller\ndef stationary_test(data):\n    #print('Is the data stationary ?')\n    dftest = adfuller(data, autolag='AIC')\n    print('Test statistic = {:.3f}'.format(dftest[0]))\n    print('P-value = {:.3f}'.format(dftest[1]))\n    print('Number of Observations = {:.0f}'.format(dftest[3]))\n    print('Critical values :')\n    for k, v in dftest[4].items():\n        print('\\t{}: {} - The data is {} stationary with {}% confidence'.format(k,v,'not' if v<dftest[0] else '',100-int(k[:-1])))\n# let's run stationary test on raw data, de-trended data, and de-seasonalized data\nprint('Is the raw data stationary ?')\nstationary_test(arima_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**AR model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# AR model\nfrom statsmodels.tsa.ar_model import AR\nimport warnings\nwarnings.filterwarnings('ignore', 'statsmodels.tsa.ar_model.AR', FutureWarning)\n\nmodel = AR(arima_data)\nar_model = model.fit()\nyhat = ar_model.predict(12,len(arima_data)+18)\n\ndataList = list(arima_data)\nyhatList = list(yhat)\n\nplt.style.use('seaborn-poster')\nplt.figure()\nplt.plot(dataList,label='Original')\nplt.plot(yhatList,ls='--',label='Predicted')\nplt.legend(loc='best')\nplt.title('AR model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\nar_rmse = np.sqrt(mean_squared_error(dataList,yhatList[0:34]))\nprint('AR RMSE: %.1f' % ar_rmse)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Initial approximation of parameters using Autocorrelation and Partial Autocorrelation Plots\n\nimport statsmodels.api as sm\nfrom statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n\nplt.figure(figsize=(12,10))\nax = plt.subplot(211)\nplot_acf(arima_data,lags=12,ax=ax)\nplot_pacf(arima_data,lags=12,ax=ax)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**ARIMA model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from statsmodels.tsa.arima_model import ARIMA\nimport statsmodels.api as sm\nimport itertools\n\nimport warnings\nfrom statsmodels.tools.sm_exceptions import HessianInversionWarning\nwarnings.simplefilter('ignore', HessianInversionWarning)\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning) \n\nbest_aic = np.inf\nbest_order = None\nbest_order_seasonal = None\nbest_model = None\np = d = q = range(3)\npdq = list(itertools.product(p,d,q))\n#seasonal_pdq = [(x[0],x[1],x[2],3) for x in list(itertools.product(p,d,q))]\noutput = []\nfor param in pdq:\n    try:\n        model = ARIMA(arima_data,\n                      order=param)\n        results = model.fit()\n        output.append('ARIMA{} - AIC:{}'.format(param,results.aic))\n        temp_aic = results.aic\n        if temp_aic<best_aic:\n            best_aic = temp_aic\n            best_order = param\n            best_model = results\n    except:\n        continue\nprint('aic: {:6.5f} | order: {}'.format(best_aic,best_order))\n#aic: 676.13694 | order: (1, 2, 2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Based on the above technique to find lowest AIC, the optimal order would be (1,2,2). However when applying oder=(1,2,2) to the ARIMA model, it throws out 'SVD not converge' error. After trial and error, I found the next best thing: order=(2,0,1)."},{"metadata":{"trusted":true},"cell_type":"code","source":"# ARIMA model\n\nmodel = ARIMA(arima_data,order=(2,0,1))\narima_model = model.fit(disp=False)\nyhat = arima_model.predict(1,len(arima_data)+6)\n\ndataList = list(arima_data)\nyhatList = list(yhat)\n\nplt.style.use('seaborn-poster')\nplt.figure()\nplt.plot(dataList,label='Original')\nplt.plot(yhatList,ls='--',label='Predicted')\nplt.legend(loc='best')\nplt.title('ARIMA model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"arima_rmse = np.sqrt(mean_squared_error(dataList,yhatList[0:34]))\nprint('ARIMA RMSE: %.1f' % arima_rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that although ARIMA model almost acurately capture the historical patterns of the data, but it didn't reflect the huge spike for the end of year data point. Let's see if SARIMAX model remedy this issue with additional parameter for seasonality."},{"metadata":{},"cell_type":"markdown","source":"**SARIMAX**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# SARIMA model\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\nmodel = SARIMAX(arima_data,\n                order=(2,0,1),\n                seasonal_order=(2,0,1,12))\nsarima_model = model.fit(disp=False)\nyhat = sarima_model.predict(1,len(arima_data)+6)\n\ndataList = list(arima_data)\nyhatList = list(yhat)\n\nplt.style.use('seaborn-poster')\nplt.figure()\nplt.plot(dataList,label='Original')\nplt.plot(yhatList,ls='--',label='Predicted')\nplt.legend(loc='best')\nplt.title('SARIMAX model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sarima_rmse = np.sqrt(mean_squared_error(dataList,yhatList[0:34]))\nprint('SARIMAX RMSE: %.1f' % sarima_rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The SARIMAX model captures the huge spike for the year-end data point, but at the expense of accuracy. \n\nAnother apparent flaw of the above AR/MA models is that they couldn't deal with hirarchical data. And it's not efficient to run an AR/MA model going through every combination of item_id and shop_id. Next for fun, I will explore a new technique called FB prophet."},{"metadata":{},"cell_type":"markdown","source":"**FB Prophet**"},{"metadata":{"trusted":true},"cell_type":"code","source":"fb_data = monthly_train_lk.groupby(['date_block_num'])['total_item_cnt'].sum()\nfb_data.index = pd.date_range(start='2013-01-01',end='2015-10-01',freq='MS')\nfb_data = fb_data.reset_index()\nfb_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from fbprophet import Prophet\nfb_data.columns=['ds','y']\nmodel = Prophet(yearly_seasonality=True)\nmodel.fit(fb_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fb_data_ext = model.make_future_dataframe(periods=6,freq='MS')\nforecast = model.predict(fb_data_ext)\nforecast[['ds','yhat','yhat_lower','yhat_upper']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataList = arima_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.style.use('seaborn-poster')\nplt.figure()\nplt.plot(dataList,label='Original')\nplt.plot(forecast['yhat'],ls='--',label='Predicted')\nplt.legend(loc='best')\nplt.title('FB Prophet model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fb_rmse = np.sqrt(mean_squared_error(dataList,forecast['yhat'][0:34]))\nprint('FB Prophet RMSE: %.1f' % fb_rmse)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The FB prophet captures both the upward trend and the spike by the end of each year. But I didn't explore how to deal with hierarchical data thru this approach. And the RMSE value is still relatively large as I didn't tune up the parameters of the FB model. For now I will leave it here, and move on to learn about other models that can deal with hierarchical data. "},{"metadata":{"trusted":true},"cell_type":"code","source":"del arima_data_df\ndel arima_data\ndel fb_data\n\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Part II : LSTM***\n\nThis part I will explore the single layer LSTM model. Why LSTM? It doesn't need level shifts and can handle non-linear relation with massive (and potentially multivariate) data."},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_data = monthly_train_lk[['date_block_num','shop_id','item_id','total_item_cnt','item_cnt_next_month']]\nlstm_data_series = lstm_data.pivot_table(index=['shop_id','item_id'],columns='date_block_num',values='total_item_cnt',fill_value=0).reset_index()\nlstm_data_series.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_data_series = pd.merge(test_sub,lstm_data_series,on=['item_id','shop_id'],how='left')\nlstm_data_series.fillna(0,inplace=True)\nlstm_data_series.drop(['shop_id','item_id'],inplace=True,axis=1)\nlstm_data_series.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Prepare train/validation/test data**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# prepare X's and y's\n# for X we will keep all columns execpt the last one \nX = np.expand_dims(lstm_data_series.values[:,:-1],axis=2)\n# the last column is our label, ie. y\ny = lstm_data_series.values[:,-1:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split train and validation among X's and y's\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.10, random_state=0)\n\nprint(\"Train set\", X_train.shape)\nprint(\"Validation set\", X_valid.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = np.expand_dims(lstm_data_series.values[:,1:],axis=2)\nprint(\"Test set\", X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Train and predict with LSTM model**"},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import LSTM,Dense,Dropout\nfrom keras import optimizers\nfrom keras.utils import plot_model\n\nseries_size = X_train.shape[1] # 12\nn_feat = X_train.shape[2] # 1\n\nepochs = 10\nbatch = 64\nlr = 0.0001\n\nlstm_model = Sequential()\nlstm_model.add(LSTM(units = 64,input_shape = (series_size,n_feat)))\nlstm_model.add(Dropout(0.4))\nlstm_model.add(Dense(1))\nlstm_model.summary()\n\nadam = optimizers.Adam(lr)\nlstm_model.compile(loss='mse',optimizer=adam)\nplot_model(lstm_model,show_shapes=True,to_file='regular_lstm.png')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_model.fit(X_train,y_train,batch_size=batch,epochs=epochs,validation_data=(X_valid,y_valid),verbose=2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Evaluate model performance**"},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_train_pred = lstm_model.predict(X_train)\nlstm_val_pred = lstm_model.predict(X_valid)\n\nprint('Train RMSE:', np.sqrt(mean_squared_error(y_train, lstm_train_pred)))\nprint('Validation RMSE:', np.sqrt(mean_squared_error(y_valid, lstm_val_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_performance_sc_plot(lstm_val_pred.flatten(),y_valid.flatten(),'Validation')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_pred = lstm_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_sub_df = pd.DataFrame(test_sub.index.values,columns=['ID'])\nlstm_sub_df['item_cnt_next_month'] = lstm_pred.clip(0.,20.)\nlstm_sub_df.to_csv('lstm_submission.csv',index=False)\nlstm_sub_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The LSTM model prediction result looks good. Although I only implemented the basic LSTM model without further tuning the model hyper parameters. And the time series data can be truncated in better shape. For now I will leave it here and move on to ensembling method."},{"metadata":{"trusted":true},"cell_type":"code","source":"del lstm_data_series\ndel lstm_data\n\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Part III : Ensemble learning***\n\nThis part I will try to ensemble several machine learning models with feature engineering techniques. It might be complicated than the two approaches earlier, though it allows the flexibility to add more features that might be helpful for prediction."},{"metadata":{},"cell_type":"markdown","source":"**Step 1: Train/validation/test split**\n\nPreviously we added lag6 and lag12 feature to the dataframe. After the first run of the selected models, it turns out that lag6 and lag12 features are not showing significant importance in the prediction. So next we will ignore lag6/lag12 features and re-split train/validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"# there are 34 blocks from train and validation sets, we will ignore the first 3 blocks (0~2) as the first 3 blocks are used to generate windowned features\n# then among the rest of the blocks available, we will split as following:\n# train size = (3~27)\n# validation size = (28-32) \n# test size = (33)\nmonthly_train_lk_new = monthly_train_lk.drop(['total_item_cnt_lag_6','total_item_cnt_lag_12'],axis=1)\n\ntrain_set = monthly_train_lk_new[(monthly_train_lk['date_block_num']>=3) & (monthly_train_lk['date_block_num']<28)].copy()\nvalidation_set = monthly_train_lk_new[(monthly_train_lk['date_block_num']>=28) & (monthly_train_lk['date_block_num']<33)].copy()\ntest_set = monthly_train_lk_new[monthly_train_lk['date_block_num']==33].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# clean the nan values in the prediction label\ntrain_set.dropna(subset=['item_cnt_next_month'],inplace=True)\nvalidation_set.dropna(subset=['item_cnt_next_month'],inplace=True)\n\ntrain_set.dropna(inplace=True)\nvalidation_set.dropna(inplace=True)\n\nprint('Train set records:', train_set.shape[0])\nprint('Validation set records:', validation_set.shape[0])\nprint('Test set records:', test_set.shape[0])\n\nprint('Train set records: %s (%.f%% of complete data)' % (train_set.shape[0], ((train_set.shape[0]/monthly_train_lk_new.shape[0])*100)))\nprint('Validation set records: %s (%.f%% of complete data)' % (validation_set.shape[0], ((validation_set.shape[0]/monthly_train_lk_new.shape[0])*100)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del monthly_train_lk\ndel monthly_train_lk_new\n\ngc.collect();","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Step 2 : Prepare train, validation and test set**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create train and validation set labels\n# we will also drop column item_category_id as it's not available in the test_sub data\nX_train = train_set.drop(['date_block_num','item_cnt_next_month','item_category_id'],axis=1)\ny_train = train_set['item_cnt_next_month'].astype(int)\nX_validation = validation_set.drop(['date_block_num','item_cnt_next_month','item_category_id'],axis=1)\ny_validation = validation_set['item_cnt_next_month'].astype(int)\n#X_test = test_set.drop(['item_cnt_next_month'],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# convert id and date related features to integer\nint_feat = ['shop_id', 'item_id', 'year', 'month']\n\nX_train[int_feat] = X_train[int_feat].astype(int)\nX_validation[int_feat] = X_validation[int_feat].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combo = pd.concat([train_set,validation_set]).drop_duplicates(subset=['shop_id','item_id'],keep='last')\nX_test = pd.merge(test_sub,combo,on=['shop_id','item_id'],how='left',suffixes=['','_'])\nX_test['year'] = 2015\nX_test['month'] = 10\nX_test.drop('item_cnt_next_month',axis=1,inplace=True)\nX_test[int_feat] = X_test[int_feat].astype(int)\nX_test = X_test[X_train.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check if there is null values in each X set\nprint(X_train.isnull().sum().sum())\nprint(X_validation.isnull().sum().sum())\nprint(X_test.isnull().sum().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# let's tackle the missing value issue in X_test\nfor item in X_test['item_id'].unique():\n    for col in X_test.columns:\n        item_median = X_test[(X_test['item_id']==item)][col].median()\n        X_test.loc[(X_test[col].isnull()) & (X_test['item_id']==item),col] = item_median\n        X_test.loc[np.isnan(X_test[col]) & (X_test['item_id']==item),col] = item_median\n# if median isn't available for a certain item, calculate median based on items in the same shop\nfor shop in X_test['shop_id'].unique():\n    for col in X_test.columns:\n        shop_median = X_test[(X_test['shop_id']==shop)][col].median()\n        X_test.loc[(X_test[col].isnull()) & (X_test['shop_id']==shop),col] = shop_median\n        X_test.loc[np.isnan(X_test[col]) & (X_test['shop_id']==shop),col] = shop_median\n\n#X_test.fillna(X_test.mean(), inplace=True)\nX_test[int_feat] = X_test[int_feat].astype(int)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# double check if there is missing value issue left\nprint(X_train.isnull().sum().sum())\nprint(X_validation.isnull().sum().sum())\nprint(X_test.isnull().sum().sum())\nprint(np.isnan(X_train).sum().sum())\nprint(np.isnan(X_validation).sum().sum())\nprint(np.isnan(X_test).sum().sum())\n\nprint(y_train.isnull().sum())\nprint(y_validation.isnull().sum())\nprint(np.isnan(y_train).sum())\nprint(np.isnan(y_validation).sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Step 3 : run individual models**\n\n***Tree based models - XGBoost***"},{"metadata":{"trusted":true},"cell_type":"code","source":"from xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\nxgb_model = XGBRegressor(max_depth=8,\n                        n_estimators=1000,\n                        min_child_weight=300,\n                        colsample_bytree=0.8,\n                        subsample=0.8,\n                        eta=0.3,\n                        seed=42)\nxgb_model.fit(X_train,y_train,\n             eval_metric='rmse',\n             eval_set=[(X_train,y_train),(X_validation,y_validation)],\n             verbose=True,\n             early_stopping_rounds=10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.rcParams['figure.figsize'] = (12,10)\nplot_importance(xgb_model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"xgb_train_pred = xgb_model.predict(X_train)\nxgb_val_pred = xgb_model.predict(X_validation)\nxgb_test_pred = xgb_model.predict(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\n\nprint('Train RMSE: ',np.sqrt(mean_squared_error(y_train,xgb_train_pred)))\nprint('Validation RMSE: ',np.sqrt(mean_squared_error(y_validation,xgb_val_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_performance_sc_plot(xgb_val_pred,y_validation,'Validation')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Tree based models - random forest***"},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove 0 importance features based on the first round model running\nrf_feat = ['shop_id', 'item_id', 'total_item_price', 'avg_item_price', 'total_item_cnt', 'avg_item_cnt', 'total_txn_cnt', 'total_revenue', 'avg_revenue', 'month', 'year', 'max_unit_item_price', 'price_change-', 'total_item_cnt_lag_1', 'total_item_cnt_lag_2', 'total_item_cnt_lag_3', 'trend', 'total_item_cnt_rolling_min','total_item_cnt_rolling_max', 'total_item_cnt_rolling_avg']\nrf_X_train = X_train[rf_feat]\nrf_X_val = X_validation[rf_feat]\nrf_X_test = X_test[rf_feat]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nrf_model = RandomForestRegressor(n_estimators=50,\n                                max_depth=7,\n                                random_state=0,\n                                n_jobs=-1)\nrf_model.fit(rf_X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Model params:', rf_model.get_params())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get feature importance\nfeat_score = pd.DataFrame(list(zip(rf_X_train.dtypes.index,rf_model.feature_importances_)),columns=['Feature','Score'])\nfeat_score = feat_score.sort_values(by='Score',ascending=False,inplace=False,kind='quicksort',na_position='last')\n\nplt.rcParams['figure.figsize'] = (12,5)\ng = feat_score.plot('Feature','Score',kind='bar',color='c')\ng.set_title('Random Forest Feature Importance')\ng.set_xlabel('')\n\nfor patch,score in zip(g.patches,feat_score['Score'].round(3)):\n    height = patch.get_height()\n    g.text(patch.get_x()+patch.get_width()/2,\n          height+0.01,\n          score,\n          ha='center')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"rf_train_pred = rf_model.predict(rf_X_train)\nrf_val_pred = rf_model.predict(rf_X_val)\nrf_test_pred = rf_model.predict(rf_X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train RMSE: ',np.sqrt(mean_squared_error(y_train,rf_train_pred)))\nprint('Validation RMSE: ',np.sqrt(mean_squared_error(y_validation,rf_val_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_performance_sc_plot(rf_val_pred,y_validation,'Validation')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"***Linear models - linear regression***"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler,MinMaxScaler\n\nnum_feat = ['total_item_price', 'avg_item_price', 'total_item_cnt', 'avg_item_cnt', 'total_txn_cnt', 'total_revenue', 'avg_revenue','unit_item_price', 'total_item_cnt_lag_1', 'total_item_cnt_lag_2', 'total_item_cnt_lag_3', 'trend','total_item_cnt_rolling_min', 'total_item_cnt_rolling_max', 'total_item_cnt_rolling_avg']\ncat_feat = ['shop_id', 'item_id', 'month', 'year']\n#X_train[int_feat] = X_train[int_feat].astype(int)\n\nlr_scaler = MinMaxScaler()\nlr_scaler.fit(X_train[num_feat])\nlr_X_train = lr_scaler.transform(X_train[num_feat])\nlr_X_val = lr_scaler.transform(X_validation[num_feat])\nlr_X_test = lr_scaler.transform(X_test[num_feat])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.linear_model import LinearRegression\n\nlr_model = LinearRegression(n_jobs=-1)\nlr_model.fit(lr_X_train,y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"feat_score = pd.DataFrame(list(zip(X_train.dtypes.index,lr_model.coef_)),columns=['Feature','Score'])\nfeat_score = feat_score.sort_values(by='Score',ascending=False,inplace=False,kind='quicksort',na_position='last')\n\nplt.rcParams['figure.figsize'] = (12,10)\ng = feat_score.plot('Feature','Score',kind='bar',color='c')\ng.set_title('Linear Regression Feature Importance')\ng.set_xlabel('')\n\n#for patch,score in zip(g.patches,feat_score['Score'].round(3)):\n#    height = patch.get_height()\n#    g.text(patch.get_x()+patch.get_width()/2,\n#          height+5,\n#          score,\n#          ha='center')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lr_train_pred = lr_model.predict(lr_X_train)\nlr_val_pred = lr_model.predict(lr_X_val)\nlr_test_pred = lr_model.predict(lr_X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train RMSE: ',np.sqrt(mean_squared_error(y_train,lr_train_pred)))\nprint('Validation RMSE: ',np.sqrt(mean_squared_error(y_validation,lr_val_pred)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_performance_sc_plot(lr_val_pred,y_validation,'Validation')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Step 4: Model ensemble**\n\nCombine predictions from all above models"},{"metadata":{"trusted":true},"cell_type":"code","source":"# train set\ncombo_train = pd.DataFrame({#'catboost':catboost_val_pred,\n                             'XGB':xgb_val_pred,\n                             'random_forest':rf_val_pred,\n                             'linear_reg':lr_val_pred})\n                             #'KNN':knn_val_pred})\n                             #'label':y_validation.values})\ncombo_train.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# test set\ncombo_test = pd.DataFrame({#'catboost':catboost_test_pred,\n                             'XGB':xgb_test_pred,\n                             'random_forest':rf_test_pred,\n                             'linear_reg':lr_test_pred})\n                             #'KNN':knn_test_pred})\ncombo_test.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combo_model = LinearRegression(n_jobs=-1)\ncombo_model.fit(combo_train,y_validation)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"combo_pred = combo_model.predict(combo_train)\nfinal_pred = combo_model.predict(combo_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('Train RMSE:', np.sqrt(mean_squared_error(combo_pred, y_validation)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_performance_sc_plot(combo_pred,y_validation,'Validation')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The result of ensembling method looks better than LSTM model. It might be that LSTM model isn't tuned well enough, or it might be that the ensemble method captures as much data feature as possible in the prediction since it combines linear, xgboost and random forest models."},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_df = pd.DataFrame(test_sub.index.values,columns=['ID'])\nsubmission_df['item_cnt_next_month'] = final_pred.clip(0.,20.)\nsubmission_df.to_csv('ensemble_submission.csv',index=False)\nsubmission_df.head(10)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}