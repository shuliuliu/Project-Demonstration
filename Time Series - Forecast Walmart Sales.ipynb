{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"markdown","source":"# Project Description\n\nThis project uses hierarchical sales data from Walmart, the worldâ€™s largest company by revenue, to forecast daily sales for the next 28 days. The data, covers stores in three US States (California, Texas, and Wisconsin) and includes item level, department, product categories, and store details. In addition, it has explanatory variables such as price, promotions, day of the week, and special events.\n\nThe goal is to predict item sales at stores in various locations for two 28-day time periods."},{"metadata":{},"cell_type":"markdown","source":"# Project Pipeline\n\n1. Load the data\n2. Preprocessing\n    * Downcasting\n    * Melting the data\n3. Exploratory data analysis\n    * Sales, revenue, category, location related data exploration\n4. Feature engineering\n    * Label encoding\n    * Lagged features\n    * Rolling window and expanding window statistics\n    * Trending stats\n    * Mean encoding\n5. Model fit and prediction\n    * Train, validation, test split\n    * Select a model (LightGBM), train the model and make prediction\n    * Define a performance evaluation function and visualize model performance\n    * Visualize feature importance stats\n    * Save output in designated format "},{"metadata":{"_kg_hide-input":false,"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly_express as px\nimport plotly.graph_objects as go\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport gc","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 1. Load the data"},{"metadata":{},"cell_type":"markdown","source":"**Files**\n* calendar.csv - Contains information about the dates on which the products are sold.\n* sell_prices.csv - Contains information about the price of the products sold per store and date.\n* sales_train_validation.csv - Contains the historical daily unit sales data per product and store [d_1 - d_1913]\n* sales_train_evaluation.csv - Includes sales [d_1 - d_1941] (labels used for the Public leaderboard)\n* sample_submission.csv - The correct format for submissions. This file tells what exactly we are trying to predict: the sales of different products at different Walmart stores for 28 forecast days."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"calendar = pd.read_csv('../input/m5-forecasting-accuracy/calendar.csv')\nprices = pd.read_csv('../input/m5-forecasting-accuracy/sell_prices.csv')\n#sales_train_val = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_validation.csv')\nsales_train = pd.read_csv('../input/m5-forecasting-accuracy/sales_train_evaluation.csv')\n#sample_sub = pd.read_csv('../input/m5-forecasting-accuracy/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"In addition to the d_0-d_1941 dates in the sales_train file, I will also add additional 28 days (d_1942-d_1969) to the sales_train dataframe, so it will work as the test dataset when I train the model and make prediction."},{"metadata":{"trusted":true},"cell_type":"code","source":"for d in range(1942,1970):\n    col = 'd_' + str(d)\n    sales_train[col] = 0\n    sales_train[col] = sales_train[col].astype(np.int16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 2. Preprocessing\n\n* **Downcasting**\n\nThe purpose of downcasting is to reduce the amount of storage used by the dataframes thus to expidite the operation performance. One lesson I learned from my last time series project that low storage leaded to poor operation performance."},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_bd = np.round(sales_train.memory_usage().sum()/(1024*1024),1)\ncalendar_bd = np.round(calendar.memory_usage().sum()/(1024*1024),1)\nprices_bd = np.round(prices.memory_usage().sum()/(1024*1024),1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def downcast(df):\n    cols = df.dtypes.index.tolist()\n    types = df.dtypes.values.tolist()\n    for i,t in enumerate(types):\n        if 'int' in str(t):\n            if df[cols[i]].min() > np.iinfo(np.int8).min and df[cols[i]].max() < np.iinfo(np.int8).max:\n                df[cols[i]] = df[cols[i]].astype(np.int8)\n            elif df[cols[i]].min() > np.iinfo(np.int16).min and df[cols[i]].max() < np.iinfo(np.int16).max:\n                df[cols[i]] = df[cols[i]].astype(np.int16)\n            elif df[cols[i]].min() > np.iinfo(np.int32).min and df[cols[i]].max() < np.iinfo(np.int32).max:\n                df[cols[i]] = df[cols[i]].astype(np.int32)\n            else:\n                df[cols[i]] = df[cols[i]].astype(np.int64)\n        elif 'float' in str(t):\n            if df[cols[i]].min() > np.finfo(np.float16).min and df[cols[i]].max() < np.finfo(np.float16).max:\n                df[cols[i]] = df[cols[i]].astype(np.float16)\n            elif df[cols[i]].min() > np.finfo(np.float32).min and df[cols[i]].max() < np.finfo(np.float32).max:\n                df[cols[i]] = df[cols[i]].astype(np.float32)\n            else:\n                df[cols[i]] = df[cols[i]].astype(np.float64)\n        elif t == np.object:\n            if cols[i] == 'date':\n                df[cols[i]] = pd.to_datetime(df[cols[i]],format='%Y-%m-%d')\n            else:\n                df[cols[i]] = df[cols[i]].astype('category')\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train = downcast(sales_train)\ncalendar = downcast(calendar)\nprices = downcast(prices)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sales_train_ad = np.round(sales_train.memory_usage().sum()/(1024*1024),1)\ncalendar_ad = np.round(calendar.memory_usage().sum()/(1024*1024),1)\nprices_ad = np.round(prices.memory_usage().sum()/(1024*1024),1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dic = {'DataFrame':['sales','calendar','prices'],\n       'Before downcasting':[sales_train_bd,calendar_bd,prices_bd],\n       'After downcasting':[sales_train_ad,calendar_ad,prices_ad]}\n\nmemory = pd.DataFrame(dic)\nmemory = pd.melt(memory, id_vars='DataFrame', var_name='Status', value_name='Memory (MB)')\nmemory.sort_values('Memory (MB)',inplace=True)\nplt.figure(figsize=(12,4))\nsns.barplot(y='DataFrame',x='Memory (MB)',data=memory,hue='Status',palette='viridis').set_title('Effect of Downcasting')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see the downcast method effectively reduced size of dataframes and saved storage."},{"metadata":{},"cell_type":"markdown","source":"* **Melting the data**\n\nThe sales_train data has 1900+ columns and each column among d_1-d_1969 represents the sales data on that day. I will first convert the wide format dataframe to long format. And then combine the sales_train data with calendar and price data, and merge them into one big table."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.melt(sales_train,id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],var_name='d',value_name='sold').dropna()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.merge(df,calendar,on='d',how='left')\ndf = pd.merge(df,prices,on=['store_id','item_id','wm_yr_wk'],how='left')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# add two columns that might be useful for EDA\ndf['week'] = df['date'].dt.weekofyear.astype(np.int16)\ndf['revenue'] = df['sell_price']*df['sold'].astype(np.float32)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del sales_train,calendar,prices\ndel memory\ndel sales_train_ad,calendar_ad,prices_ad,sales_train_bd,calendar_bd,prices_bd\n\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above, null values exist in price column and event columns. Missing value in event columns are ligit and can be handled via encoding. The missing values in price column can just be dropped. I will do it later after feature engineering is done."},{"metadata":{},"cell_type":"markdown","source":"# 3. Exploratory data analysis"},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = df.groupby(['state_id','cat_id'],as_index=False)['item_id'].count().dropna()\nplt.figure(figsize=(12,6))\nsns.barplot(y='state_id',x='item_id',data=temp,hue='cat_id',palette='viridis').set_title('Distribution of categories')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above, Food category has the largest variety of items, and Hobbies category has the smallest variety of items."},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = df.groupby(['state_id','cat_id','item_id'],as_index=False)['sell_price'].mean().dropna()\nplt.figure(figsize=(12,6))\nsns.boxplot(x='state_id',y='sell_price',data=temp,hue='cat_id',palette='viridis').set_title('Distribution of item price (median) - by category')\nplt.legend(loc='upper left',ncol=temp['cat_id'].nunique(),mode='expand')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = df.groupby(['state_id','store_id','cat_id','item_id'],as_index=False)['sell_price'].mean().dropna()\nplt.figure(figsize=(12,6))\nsns.boxplot(x='store_id',y='sell_price',data=temp,hue='cat_id',palette='viridis').set_title('Distribution of item price (median) - by category cross store')\nplt.legend(loc='upper left',ncol=temp['cat_id'].nunique(),mode='expand')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above, items under food category has relatively the lowest price and smallest price range. Items under Hobbies has the largest price range. Items under Household has highest average price and a smaller price range compared to Hobbies item."},{"metadata":{"trusted":true},"cell_type":"code","source":"#temp = df.groupby(['date','state_id','store_id'],as_index=False)['sold'].sum().dropna()\ntemp = df.groupby(['date','d','cat_id'],as_index=False)['sold'].sum().dropna()\ntemp_pivot = temp.pivot(values='sold',columns='cat_id',index='date')\ntemp_pivot.head()\n\nplt.figure(figsize=(12,5))\nplt.plot(temp_pivot['FOODS'].rolling(window=60,center=False).mean(),label='60d rolling mean-FOODS')\nplt.plot(temp_pivot['HOBBIES'].rolling(window=60,center=False).mean(),label='60d rolling mean-HOBBIES')\nplt.plot(temp_pivot['HOUSEHOLD'].rolling(window=60,center=False).mean(),label='60d rolling mean-HOUSEHOLD')\nplt.title('Trend of Items Sold across categories')\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above, the sales of Hobbies items increased at a very slow pace compared to foods and households."},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = df.pivot_table(columns='month',index=['state_id','cat_id','year'],values='sold',aggfunc=np.sum)\nplt.figure(figsize=(20,18))\nsns.heatmap(temp,cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above, there is a very slight trend that the sales of all three categories are more popular in summer than in winter."},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = df.groupby(['date','state_id','store_id'],as_index=False)['sold'].sum().dropna()\nfig, ax = plt.subplots(figsize=(12,5))\nsns.barplot(ax=ax,y='store_id',x='sold',data=temp,hue='state_id',palette='viridis').set_title('Distribution of item sales - cross stores')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above, store CA_3, TX_2 and WI_2 sell the most items in CA, TX and WI, respectively. "},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = df.groupby(['state_id','store_id','cat_id'],as_index=False)['revenue'].sum().dropna()\ntemp_pivot = pd.pivot_table(temp,values='revenue',columns='cat_id',index='store_id',margins=True,aggfunc='sum',).drop('All',0).reset_index()\n\ntotal = temp_pivot['All']\ntemp_pivot = temp_pivot.iloc[:, 0:4]\n\ncolors = ['#006D2C','#31A354','#74C476']\ntemp_pivot.plot(x='store_id',kind='bar',stacked=True,title='Revenue % breakdown of categories',color=colors,mark_right=True,figsize=(12,6))\n\npercent = temp_pivot[temp_pivot.columns[1:]].div(total, 0)*100\nfor n in percent:\n    for i, (cs,col,pc,tot) in enumerate(zip(temp_pivot.iloc[:,1:].cumsum(1)[n],temp_pivot[n],percent[n],total)):\n        #plt.text(tot, i, str(tot), va='center')\n        plt.text(i,cs-col/2,str(np.round(pc,1))+'%',va='center',ha='center',fontsize=8,c='white')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above, store CA_3, TX_2 and WI_2 contributes the most revenue in CA, TX and WI, respectively. Also, food items generally contributes more than 50% revenue, while hobbies items generally contribute less than 15% revenue."},{"metadata":{"trusted":true},"cell_type":"code","source":"temp = df.pivot_table(columns='week',index=['year','weekday'],values='revenue',aggfunc=np.sum)\nplt.figure(figsize=(20,14))\nsns.heatmap(temp,cmap='coolwarm')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above, generally there is a spike in revenue over Saturdays and Sundays. Also there is a slight trend that on 48th and 52th weeks the overall revenue in Walmart drops, perhaps over the two big year-end holidays people tend to go to fancy stores for shopping other than Walmart. Or people are traveling around and need not go grocery shopping."},{"metadata":{},"cell_type":"markdown","source":"Also I noticed there are columns called snap_CA, snap_TX and snap_WI. What's SNAP? SNAP is a nutrition assistance benefit provided by US federal government. It provides low income families and individuals with an Electronic Benefits Transfer debit card to purchase food products. In many states, the monetary benefits are dispersed to people across 10 days of the month and on each of these days 1/10 of the people will receive the benefit on their card.\n\nBack to the 'snap' columns in the dataframe, value 1 means it's a SNAP day and value 0 means it's a non-SNAP day. Next I will explore how sales change among SNAP and non-SNAP days."},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_break(df):\n    idx = pd.date_range(df.date.dt.date.min(),df.date.dt.date.max())\n    df = df.set_index('date')\n    df = df.reindex(idx)\n    df.reset_index(inplace=True)\n    df.rename(columns={'index':'date'},inplace=True)\n    return df\n\ndef plot_snap(df,state,metric):\n    total = df[df['state_id']==state]\n    food_total = total[total['cat_id']=='FOODS']\n    \n    total = total.groupby(['date','snap_'+state],as_index=False)['sold','revenue'].sum()\n    total_snap = total[total['snap_'+state]==1]\n    total_non_snap = total[total['snap_'+state]==0]\n    food_total = food_total.groupby(['date','snap_'+state],as_index=False)['sold','revenue'].sum()\n    food_snap = food_total[food_total['snap_'+state]==1]\n    food_non_snap = food_total[food_total['snap_'+state]==0]\n    \n    total_snap = plot_break(total_snap)\n    total_non_snap = plot_break(total_non_snap)\n    food_snap = plot_break(food_snap)\n    food_non_snap = plot_break(food_non_snap)\n    \n    fig = go.Figure()\n    fig.add_trace(go.Scatter(x=total_non_snap['date'],y=total_non_snap[metric],name='Total '+metric+'(Non-SNAP)'))\n    fig.add_trace(go.Scatter(x=total_snap['date'],y=total_snap[metric],name='Total '+metric+'(SNAP)'))\n    fig.add_trace(go.Scatter(x=food_non_snap['date'],y=food_non_snap[metric],name='Food '+metric+'(Non-SNAP)'))\n    fig.add_trace(go.Scatter(x=food_snap['date'],y=food_snap[metric],name='Food '+metric+'(SNAP)'))\n    fig.update_yaxes(title_text='Total items sold' if metric=='sold' else 'Total revenue($)')\n    fig.update_layout(template='seaborn',title=state)\n    fig.update_layout(legend=dict(\n                    orientation=\"h\",\n                    yanchor=\"bottom\",\n                    y=1.02,\n                    xanchor=\"right\",\n                    x=1\n                    ))\n    \n    return fig","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig1 = plot_snap(df,'CA','sold')\nfig1.show()\nfig2 = plot_snap(df,'CA','revenue')\nfig2.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above, I ran the same for TX and WI, basically on SNAP days both sales and revenues are higher than those on non-SNAP days. "},{"metadata":{"trusted":true},"cell_type":"code","source":"del temp, temp_pivot\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 4. Feature engineering\n\n* **Label encoding**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Store the categories along with their codes\ndf_id = dict(zip(df.id.cat.codes,df.id))\ndf_store_id = dict(zip(df.store_id.cat.codes,df.store_id))\n#df_item_id = dict(zip(df.item_id.cat.codes,df.item_id))\n#df_dept_id = dict(zip(df.dept_id.cat.codes,df.dept_id))\n#df_cat_id = dict(zip(df.cat_id.cat.codes,df.cat_id))\n#df_state_id = dict(zip(df.state_id.cat.codes,df.state_id))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# transform column 'd' so it can be stored as int type\ndf.d = df['d'].apply(lambda x: x.split('_')[1]).astype(np.int16)\n# encode the categorical columns\ncols = df.dtypes.index.tolist()\ntypes = df.dtypes.values.tolist()\nfor i,t in enumerate(types):\n    if t.name == 'category':\n        df[cols[i]] = df[cols[i]].cat.codes        ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# drop column 'date' as this information has been present in column 'd'\ndf.drop(['date'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Lagged features**"},{"metadata":{"trusted":true},"cell_type":"code","source":"lags = [1,3,7,14,28]\nfor lag in lags:\n    col = ('sold_lag_%s' % lag)\n    df[col] = df.groupby(['id','item_id','dept_id','cat_id','store_id','state_id'],as_index=False)['sold'].shift(lag).astype(np.float16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Rolling and expanding statistics**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# rolling stats with window=7 and 28\nfunc_roll_7 = lambda x: x.rolling(window=7).mean()\nfunc_roll_28 = lambda x: x.rolling(window=28).mean()\ndf['sold_rolling7_mean'] = df.groupby(['id', 'item_id','dept_id','cat_id','store_id','state_id'])['sold'].apply(func_roll_7).astype(np.float16)\ndf['sold_rolling28_mean'] = df.groupby(['id', 'item_id','dept_id','cat_id','store_id','state_id'])['sold'].apply(func_roll_28).astype(np.float16)\ndf['revenue_rolling7_mean'] = df.groupby(['id', 'item_id','dept_id','cat_id','store_id','state_id'])['revenue'].apply(func_roll_7).astype(np.float16)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# expanding stats\nfunc_exp = lambda x: x.expanding(2).mean()\ndf['sold_expanding_mean'] = df.groupby(['id', 'item_id','dept_id','cat_id','store_id','state_id'])['sold'].apply(func_exp).astype(np.float16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Trending stats**\n\nFor now I will be skipping the trending features due to limited memory."},{"metadata":{"trusted":true},"cell_type":"code","source":"# the trending stat will have a positive value if the daily items sold are greater than the entire duration average ( d_1 - d_1969 ), otherwise it will have a negative value\n#df['sold_daily_avg'] = df.groupby(['d','id', 'item_id','dept_id','cat_id','store_id','state_id'])['sold'].mean().astype(np.float16)\n#df['sold_avg'] = df.groupby(['id', 'item_id','dept_id','cat_id','store_id','state_id'])['sold'].mean().astype(np.float16)\n#df['sold_trend'] = (df['sold_daily_avg']-df['sold_avg']).astype(np.float16)\n#df.drop(['sold_daily_avg','sold_avg'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* **Mean encoding**\n\nFrom a mathematical point of view, mean encoding represents a probability of your target variable, conditional on each value of the feature. In a way, it embodies the target variable in its encoded value. For example, we can take average sales by combining different categorical features:"},{"metadata":{"trusted":true},"cell_type":"code","source":"#item\n# df['sold_mean_item'] = df.groupby('item_id')['sold'].mean().astype(np.float16)\n#category & department\n# df['sold_mean_cat_dept'] = df.groupby(['cat_id','dept_id'])['sold'].mean().astype(np.float16)\n#state, store and category\n# df['sold_mean_state_store_cat'] = df.groupby(['state_id','store_id','cat_id'])['sold'].mean().astype(np.float16)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"However one downside with mean encoding is that it may cause data leakage as these average values contain information from both train set and validation set. For now I will skip the mean encoding features."},{"metadata":{"trusted":true},"cell_type":"code","source":"# after creating lagged features and calculating mean values, now I will get rid of the first 28 days' data to avoid null values\ndf = df[df['d']>=28]\ndf.dropna(inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print('\\ndf shape: ',df.shape)\nprint('\\ndf null values: ',df.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# 5. Model fit and prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Since the goal is to predict item sales at stores in various locations for two 28-day time periods, in other words the output will be (N store * 28 day) dimension, I will do train/test data split for each store respectively, then make prediction/evaluation separately."},{"metadata":{"trusted":true},"cell_type":"code","source":"# split the data: there are 1969 dates in the sales_validation file, I will use:\n# d1-d1913: 1913 days as train\n# d1914-1941: 28 days as validation\n# d1942-1969: 28 days as test\nvalid = df[(df['d']>=1914)&(df['d']<1942)][['id','d','sold']]\ntest = df[df['d']>=1942][['id','d','sold']]\ny_pred_val = valid['sold']\ny_pred_test = test['sold']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Why LightGBM? In the last time series project, I used XGBoost. LightGBM improves on XGBoost. In a LightGBM paper ([paper link here](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree)), it uses XGBoost as a baseline and outperforms it in training speed and the dataset sizes it can handle. In this project I will try to play with LightGBM and see how it works."},{"metadata":{"trusted":true},"cell_type":"code","source":"from lightgbm import LGBMRegressor\nimport joblib\n\nstores = df['store_id'].unique().tolist()\nfor store in stores:\n    df_sub = df[df['store_id']==store]\n    \n    X_train,y_train = df_sub[df_sub['d']<1914].drop('sold',axis=1),df_sub[df_sub['d']<1914]['sold']\n    X_valid,y_valid = df_sub[(df_sub['d']>=1914)&(df_sub['d']<1942)].drop('sold',axis=1),df_sub[(df_sub['d']>=1914)&(df_sub['d']<1942)]['sold']\n    X_test = df_sub[df_sub['d']>=1942].drop('sold',axis=1)\n    \n    model = LGBMRegressor(n_estimators=1000,\n                          learning_rate=0.3,\n                          subsample=0.8,\n                          colsample_bytree=0.8,\n                          max_depth=8,\n                          num_leaves=50,\n                          #objective='poisson',\n                          min_child_weight=300)\n    \n    print('##### Prediction for store: {} #####'.format(df_store_id[store]))\n    model.fit(X_train,y_train,\n              eval_set=[(X_train,y_train),(X_valid,y_valid)],\n              eval_metric='rmse',\n              verbose=20,\n              early_stopping_rounds=20)\n    y_pred_val[X_valid.index] = model.predict(X_valid)\n    y_pred_test[X_test.index] = model.predict(X_test)\n    \n    filename = 'model'+str(df_store_id[store])+'.pkl'\n    # save model \n    joblib.dump(model,filename)\n    del model,X_train,y_train,X_valid,y_valid\n    gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(y_pred_val.head())\nprint(df[(df['d']>=1914)&(df['d']<1942)]['sold'].head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define a performance evaluation function and visualize the performance of the model\n# The closer the points are to the middle dashed line the better are the predictions.\nfrom numpy.polynomial.polynomial import polyfit\n\ndef model_performance_sc_plot(predictions,labels,title):\n    min_val = max(max(predictions),max(labels))\n    max_val = min(min(predictions),min(labels))\n    performance = pd.DataFrame({'Label':labels,'Prediction':predictions})\n    \n    b, m = polyfit(performance['Prediction'], performance['Label'], 1)\n    # plot the data\n    plt.figure(figsize=(8,6))\n    plt.plot([min_val,max_val],[min_val,max_val],'m--')\n    plt.plot(performance['Prediction'],performance['Label'],'.')\n    plt.plot(performance['Prediction'],b+m*performance['Prediction'],'-')\n    plt.title(title)\n\nmodel_performance_sc_plot(y_pred_val,df[(df['d']>=1914)&(df['d']<1942)]['sold'],'Validation')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From the above, we can see that data points  are almost evenly distributed along the dashed line, which proves that LightGBM model can also provide top accuracy."},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\n\nfeat_importance = pd.DataFrame()\nfeatures = [f for f in df.columns if f != 'sold']\nfor file in os.listdir('/kaggle/working'):\n    if 'model' in file:\n        # load model\n        model = joblib.load(file)\n        store = pd.DataFrame()\n        store['feature'] = features\n        store['importance'] = model.feature_importances_\n        store['store'] = file[5:9]\n        feat_importance =  pd.concat([feat_importance,store],axis=0)\n\nfeat_importance.T","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"cols = feat_importance[['feature','importance']].groupby('feature').mean().sort_values(by='importance',ascending=False)[:20].index\nbest_feat = feat_importance.loc[feat_importance['feature'].isin(cols)]\n\nplt.figure(figsize=(8,10))\nsns.barplot(x='importance',y='feature',data=best_feat.sort_values(by='importance',ascending=False))\nplt.title('LightGBM Feature Importance (average across stores)')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Generate submission file**"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Get the actual validation results\nvalid['sold'] = y_pred_val\nvalidation = valid[['id','d','sold']]\nvalidation = pd.pivot(validation, index='id', columns='d', values='sold').reset_index()\nvalidation.columns=['id'] + ['F' + str(i + 1) for i in range(28)]\nvalidation.id = validation.id.map(df_id).str.replace('evaluation','validation')\n\n#Get the evaluation results\ntest['sold'] = y_pred_test\nevaluation = test[['id','d','sold']]\nevaluation = pd.pivot(evaluation, index='id', columns='d', values='sold').reset_index()\nevaluation.columns=['id'] + ['F' + str(i + 1) for i in range(28)]\n#Remap the category id to their respective categories\nevaluation.id = evaluation.id.map(df_id)\n\n#Prepare the submission\nsubmit = pd.concat([validation,evaluation]).reset_index(drop=True)\nsubmit.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}